{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the undergraduate monograph of Ufal-Penedo Information Systems Course graduates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import re, traceback, csv, pandas as pd, time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from unidecode import unidecode\n",
    "from playwright.async_api import async_playwright, expect\n",
    "from twisted.internet.error import TCPTimedOutError, TimeoutError\n",
    "import playwright._impl._errors as errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Spider class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderUfalSI:\n",
    "    def __init__(self, url):\n",
    "        self.__url_base = url\n",
    "        self.__max_attempts = 2\n",
    "        self.__user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                             \"Chrome/122.0.0.0 Safari/537.36 OPR/108.0.0.0\")\n",
    "        self.__playwright = None\n",
    "        self.__browser = None\n",
    "        self.__page = None\n",
    "\n",
    "    async def __get_html(self, url, css_selector=None, to_close=True):\n",
    "        if self.__playwright is None:\n",
    "            self.__playwright = await async_playwright().start()\n",
    "        if self.__browser is None or not self.__browser.is_connected():\n",
    "            self.__browser = await self.__playwright.chromium.launch(headless=True, args=[\"--start-maximized\"])\n",
    "        if self.__page is None or self.__page.is_closed():\n",
    "            self.__page = await self.__browser.new_page(user_agent=self.__user_agent)\n",
    "            # self.__page = await self.__browser.new_page()\n",
    "        await self.__page.goto(url)\n",
    "        if css_selector is not None:\n",
    "            await self.__page.wait_for_selector(css_selector)\n",
    "        html = await self.__page.content()\n",
    "        if to_close:\n",
    "            await self.__browser.close()\n",
    "            await self.__playwright.stop()\n",
    "        return html\n",
    "\n",
    "    async def __collect_links(self, css_sel):\n",
    "        await self.__page.locator(css_sel).wait_for()\n",
    "        css_sel = f\"{css_sel} > div[class='card-wrapper'] > a\"\n",
    "        links = self.__page.locator(css_sel)\n",
    "        links = [await link.get_attribute(\"href\") for link in await links.all()]\n",
    "        return links\n",
    "\n",
    "    async def __collect_data(self, url):\n",
    "        await self.__page.goto(url)\n",
    "        html = await self.__page.content()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        rows = soup.css.select(\"div[class='list-group-item'] > div.row\")\n",
    "        record = {re.sub(r\"\\s+\", \" \", r.css.select_one(\"strong\").get_text(), flags=re.IGNORECASE).strip():\n",
    "                    re.sub(r\"\\s+\", \" \", r.css.select_one(\"div\").get_text(), flags=re.IGNORECASE).strip()\n",
    "                 for r in rows}\n",
    "        return record\n",
    "\n",
    "    async def extract_data(self, num_attempt=0):\n",
    "        try:\n",
    "            links = list()\n",
    "            css_sel = \"section#content > div:nth-child(2) > div > div:nth-child(2)\"\n",
    "            await self.__get_html(self.__url_base, css_sel, False)\n",
    "            while(True):\n",
    "                links.extend(await self.__collect_links(css_sel))\n",
    "                button = self.__page.locator(\"ul.pagination > li:nth-child(3) > a\")\n",
    "                if await button.is_visible():\n",
    "                    await button.click()\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    break\n",
    "            records = list()\n",
    "            for link in links:\n",
    "                link = urljoin(self.__url_base, link)\n",
    "                records.append(await self.__collect_data(link))\n",
    "            await self.__browser.close()\n",
    "            await self.__playwright.stop()\n",
    "            return records\n",
    "        except (errors.TimeoutError, errors.TargetClosedError, errors.Error, AttributeError, Exception,\n",
    "                TCPTimedOutError, TimeoutError) as e:\n",
    "            print(f\"[ERROR-DEBUG] {e}: {self.__url_base}\")\n",
    "            print(\"\".join(traceback.format_tb(e.__traceback__)))\n",
    "            if num_attempt <= self.__max_attempts:\n",
    "                num_attempt += 1\n",
    "                print(f\"Number of attempting in 'extract_data': {num_attempt}\")\n",
    "                await self.extract_data(num_attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T21:58:56.282988Z",
     "start_time": "2023-11-12T21:58:56.281174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determining the URL of target page.\n",
    "url = \"https://ud10.arapiraca.ufal.br/repositorio/publicacoes/?curso_id__id=27&page_num=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting webdriver.\n",
    "spider = SpiderUfalSI(url)\n",
    "\n",
    "# Collecting the data.\n",
    "records = await spider.extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T21:59:44.880941Z",
     "start_time": "2023-11-12T21:59:44.854963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the data into a CSV file.\n",
    "df = pd.DataFrame(records)\n",
    "cols = {\"Autor(a)\": \"authors\", \"Ano de publicação\": \"publication_year\", \"Data da defesa\": \"defense_date\",\n",
    "        \"Curso/Outros\": \"course\", \"Número de folhas\": \"num_pages\", \"Tipo\": \"type_document\", \"Local\": \"defense_local\",\n",
    "        \"Resumo\": \"abstract_ptbr\", \"Abstract\": \"abstract_en\", \"Orientador(a)\": \"advisor\", \"Coorientador(a)\": \"co-advisor\",\n",
    "        \"Banca Examinadora\": \"examining_board\", \"Palavras-chave\": \"keywords\", \"Áreas do Conhecimento/Localização\": \"subject_areas\",\n",
    "        \"Categorias CNPQ\": \"CNPq_categories\", \"Anexos\": \"attachments\", \"Visualizações\": \"views\", \"Observações\": \"observations\"}\n",
    "df.rename(columns=cols, inplace=True)\n",
    "df.to_csv(\"data_ufal.csv\", sep=\",\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
